{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Spam Classifier with Naive Bayes\n",
    "\n",
    "Our Mission\n",
    "Spam detection is one of the major applications of Machine Learning in the interwebs today. Pretty much all of the major email service providers have spam detection systems built in and automatically classify such mail as 'Junk Mail'.\n",
    "\n",
    "In this mission we will be using the Naive Bayes algorithm to create a model that can classify dataset SMS messages as spam or not spam, based on the training we give to the model. It is important to have some level of intuition as to what a spammy text message might look like. Usually they have words like 'free', 'win', 'winner', 'cash', 'prize' and the like in them as these texts are designed to catch your eye and in some sense tempt you to open them. Also, spam messages tend to have words written in all capitals and also tend to use a lot of exclamation marks. To the recipient, it is usually pretty straightforward to identify a spam text and our objective here is to train a model to do that for us!\n",
    "\n",
    "Being able to identify spam messages is a binary classification problem as messages are classified as either 'Spam' or 'Not Spam' and nothing else. Also, this is a supervised learning problem, as we will be feeding a labelled dataset into the model, that it can learn from, to make future predictions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "OverviewÂ¶\n",
    "This project has been broken down in to the following steps:\n",
    "\n",
    "Step 0: Introduction to the Naive Bayes Theorem\n",
    "Step 1.1: Understanding our dataset\n",
    "Step 1.2: Data Preprocessing\n",
    "Step 2.1: Bag of Words(BoW)\n",
    "Step 2.2: Implementing BoW from scratch\n",
    "Step 2.3: Implementing Bag of Words in scikit-learn\n",
    "Step 3.1: Training and testing sets\n",
    "Step 3.2: Applying Bag of Words processing to our dataset.\n",
    "Step 4.1: Bayes Theorem implementation from scratch\n",
    "Step 4.2: Naive Bayes implementation from scratch\n",
    "Step 5: Naive Bayes implementation using scikit-learn\n",
    "Step 6: Evaluating our model\n",
    "Step 7: Conclusion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Understanding Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"smsspamcollection.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File Name                                             Modified             Size\nSMSSpamCollection                              2011-03-15 22:36:02       477907\nreadme                                         2011-04-18 14:53:56         5868\n"
     ]
    }
   ],
   "source": [
    "with ZipFile(file_name, 'r') as zip:\n",
    "    zip.printdir()\n",
    "    zip.extractall() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['.git', 'data.csv', 'decision_trees.ipynb', 'Naivebayes_spamClassifier.ipynb', 'Naive_bayes.ipynb', 'readme', 'SMSSpamCollection', 'smsspamcollection.zip', 'titanic_data.csv', 'titanic_survival_exploration.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n",
    "file_name_comp = \"SMSSpamCollection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>sms_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    " df = pd.read_table(file_name_comp,sep='\\t',header=None,names=['label','sms_message'])\n",
    " df.head()"
   ]
  },
  {
   "source": [
    "## Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5572,)"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "df[\"label\"] = df.label.map({'ham':0, 'spam':1})\n",
    "df[\"label\"].shape"
   ]
  },
  {
   "source": [
    "## Bag of Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['hello,!!! how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"
     ]
    }
   ],
   "source": [
    "# Example \n",
    "# Convert all strings to their lower case form.\n",
    "documents = ['Hello,!!! how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "lower_case_documents = []\n",
    "for i in documents:\n",
    "    lower_case_documents.append(i.lower())\n",
    "print(lower_case_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['hello how are you', 'win money win from home', 'call me now', 'hello call hello you tomorrow']\n"
     ]
    }
   ],
   "source": [
    "# Removing all punctuations\n",
    "sans_punctuation_documents = []\n",
    "import string\n",
    "\n",
    "for i in lower_case_documents:\n",
    "    # sans_punctuation_documents.append(i.translate(None, string.punctuation))\n",
    "    sans_punctuation_documents.append(i.translate(str.maketrans('', '', string.punctuation)))\n",
    "print(sans_punctuation_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['hello', 'how', 'are', 'you'], ['win', 'money', 'win', 'from', 'home'], ['call', 'me', 'now'], ['hello', 'call', 'hello', 'you', 'tomorrow']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization \n",
    "preprocessed_documents = []\n",
    "for i in sans_punctuation_documents:\n",
    "    preprocessed_documents.append(i.split(\" \"))\n",
    "print(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}),\n Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}),\n Counter({'call': 1, 'me': 1, 'now': 1}),\n Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"
     ]
    }
   ],
   "source": [
    "# Count Frequency of each word in a document\n",
    "Frequency_list = []\n",
    "import pprint\n",
    "from collections import Counter\n",
    "\n",
    "for i in preprocessed_documents:\n",
    "    Frequency_list.append(Counter(i))\n",
    "pprint.pprint(Frequency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing BoW in scikit-learn using count vectorizer\n",
    "documents = ['Hello, how are you!',\n",
    "                'Win money, win from home.',\n",
    "                'Call me now.',\n",
    "                'Hello, Call hello you tomorrow?']\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vector = CountVectorizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "count_vector.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['are',\n",
       " 'call',\n",
       " 'from',\n",
       " 'hello',\n",
       " 'home',\n",
       " 'how',\n",
       " 'me',\n",
       " 'money',\n",
       " 'now',\n",
       " 'tomorrow',\n",
       " 'win',\n",
       " 'you']"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "count_vector.get_feature_names()"
   ]
  },
  {
   "source": [
    "doc_array = count_vector.transform(documents).toarray()\n",
    "doc_array"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 60,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
       "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
       "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
       "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
       "3    0     1     0      2     0    0   0      0    0         1    0    1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>are</th>\n      <th>call</th>\n      <th>from</th>\n      <th>hello</th>\n      <th>home</th>\n      <th>how</th>\n      <th>me</th>\n      <th>money</th>\n      <th>now</th>\n      <th>tomorrow</th>\n      <th>win</th>\n      <th>you</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "frequency_matrix = pd.DataFrame(doc_array,columns = count_vector.get_feature_names())\n",
    "frequency_matrix"
   ]
  },
  {
   "source": [
    "# Training and Testing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of rows in the total set: 5572\nNumber of rows in the training set: 4179\nNumber of rows in the test set: 1393\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], \n",
    "                                                    df['label'], \n",
    "                                                    random_state=1)\n",
    "print('Number of rows in the total set: {}'.format(df.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the count vector method\n",
    "count_vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<4179x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55209 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "# Fit the trainig data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<1393x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17604 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "testing_data = count_vector.transform(X_test)\n",
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The probability of getting a positive test result P(Pos) is: 0.10799999999999998\n"
     ]
    }
   ],
   "source": [
    "# P(D)\n",
    "p_diabetes = 0.01\n",
    "\n",
    "# P(~D)\n",
    "p_no_diabetes = 0.99\n",
    "\n",
    "# Sensitivity or P(Pos|D)\n",
    "p_pos_diabetes = 0.9\n",
    "\n",
    "# Specificity or P(Neg|~D)\n",
    "p_neg_no_diabetes = 0.9\n",
    "\n",
    "# P(Pos)\n",
    "p_pos = (p_diabetes  *  p_pos_diabetes ) + (p_no_diabetes * (1 - p_neg_no_diabetes))\n",
    "print('The probability of getting a positive test result P(Pos) is: {}'.format(p_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Probability of an individual having diabetes, given that that individual got a positive test result is: 0.08333333333333336\n"
     ]
    }
   ],
   "source": [
    "p_diabetes_pos = (p_diabetes *  p_pos_diabetes )/ p_pos\n",
    "print('Probability of an individual having diabetes, given that that individual got a positive test result is:\\\n",
    "',format(p_diabetes_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Probability of an individual not having diabetes, given that that individual got a positive test result is: 8.250000000000002\n"
     ]
    }
   ],
   "source": [
    "p_pos_no_diabetes = 1 - (0.9)\n",
    "\n",
    "p_no_diabetes_pos = (p_no_diabetes * p_pos_diabetes) / p_pos\n",
    "\n",
    "print ('Probability of an individual not having diabetes, given that that individual got a positive test result is:'\\\n",
    ",p_no_diabetes_pos)"
   ]
  },
  {
   "source": [
    "### Naive Bayes implementation from scratch for multiple variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.005000000000000001\n"
     ]
    }
   ],
   "source": [
    "# P(J)\n",
    "p_j = 0.5\n",
    "\n",
    "# P(F/J)\n",
    "p_j_f = 0.1\n",
    "\n",
    "# P(I/J)\n",
    "p_j_i = 0.1\n",
    "\n",
    "p_j_text = p_j * p_j_i * p_j_f\n",
    "print(p_j_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.06999999999999999\n"
     ]
    }
   ],
   "source": [
    "# P(G)\n",
    "p_g = 0.5\n",
    "\n",
    "# P(F/G)\n",
    "p_g_f = 0.7\n",
    "\n",
    "# P(I/G)\n",
    "p_g_i = 0.2\n",
    "\n",
    "p_g_text = p_g * p_g_f * p_g_i\n",
    "print(p_g_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Probability of words freedom and immigration being said are:  0.075\n"
     ]
    }
   ],
   "source": [
    "p_f_i = p_g_text + p_j_text\n",
    "print('Probability of words freedom and immigration being said are: ', format(p_f_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The probability of Jill Stein saying the words Freedom and Immigration:  0.06666666666666668\n"
     ]
    }
   ],
   "source": [
    "p_j_fi = p_j_text / p_f_i\n",
    "print('The probability of Jill Stein saying the words Freedom and Immigration: ', format(p_j_fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The probability of Gary Johnson saying the words Freedom and Immigration:  0.9333333333333332\n"
     ]
    }
   ],
   "source": [
    "p_g_fi = p_g_text / p_f_i\n",
    "print('The probability of Gary Johnson saying the words Freedom and Immigration: ', format(p_g_fi))"
   ]
  },
  {
   "source": [
    "### Naive Bayes Implementation using Scikit-learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "predictions = naive_bayes.predict(testing_data)\n",
    "predictions"
   ]
  },
  {
   "source": [
    "## Evaluating our model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy : ratio of the number of correct predictions to the total number of predictions \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy score:  0.9885139985642498\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision score:  0.9720670391061452\n"
     ]
    }
   ],
   "source": [
    "## Precision : ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification) [True Positives/(True Positives + False Positives)]\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Recall score:  0.9405405405405406\n"
     ]
    }
   ],
   "source": [
    "## a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of [True Positives/(True Positives + False Negatives)]\n",
    "print('Recall score: ', format(recall_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score:  0.9560439560439562\n"
     ]
    }
   ],
   "source": [
    "## It merges the recall and precison and takes an avergae raning from 0 to 1\n",
    "print('F1 score: ', format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}